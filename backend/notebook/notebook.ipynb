{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index recipes already exists with 0 documents\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "from elasticsearch import Elasticsearch\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "import time\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Elasticsearch connection\n",
    "app.es_client = Elasticsearch(\n",
    "    \"https://localhost:9200\", \n",
    "    basic_auth=(\"elastic\", \"+zu*TMbwCT-I9_fi3-L4\"), \n",
    "    ca_certs=\"~/http_ca.crt\",\n",
    "    verify_certs=True\n",
    ")\n",
    "\n",
    "# Index name\n",
    "INDEX_NAME = \"recipes\"\n",
    "PICKLE_PATH = \"resource/pickles/recipes_index.pkl\"\n",
    "\n",
    "def create_index():\n",
    "    \"\"\"Create Elasticsearch index with BM25 settings if it doesn't exist\"\"\"\n",
    "    # Check if index exists\n",
    "    if not app.es_client.indices.exists(index=INDEX_NAME):\n",
    "        # Define BM25 settings\n",
    "        settings = {\n",
    "            \"settings\": {\n",
    "                \"number_of_shards\": 1,\n",
    "                \"number_of_replicas\": 0,\n",
    "                \"similarity\": {\n",
    "                    \"default\": {\n",
    "                        \"type\": \"BM25\",\n",
    "                        \"b\": 0.75,\n",
    "                        \"k1\": 1.2\n",
    "                    }\n",
    "                },\n",
    "                \"analysis\": {\n",
    "                    \"analyzer\": {\n",
    "                        \"recipe_analyzer\": {\n",
    "                            \"type\": \"custom\",\n",
    "                            \"tokenizer\": \"standard\",\n",
    "                            \"filter\": [\"lowercase\", \"stop\", \"snowball\"]\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"mappings\": {\n",
    "                \"properties\": {\n",
    "                    \"RecipeId\": {\"type\": \"keyword\"},\n",
    "                    \"Name\": {\n",
    "                        \"type\": \"text\",\n",
    "                        \"analyzer\": \"recipe_analyzer\",\n",
    "                        \"fields\": {\n",
    "                            \"keyword\": {\"type\": \"keyword\"}\n",
    "                        }\n",
    "                    },\n",
    "                    \"Description\": {\"type\": \"text\", \"analyzer\": \"recipe_analyzer\"},\n",
    "                    \"RecipeCategory\": {\"type\": \"keyword\"},\n",
    "                    \"Keywords\": {\"type\": \"text\", \"analyzer\": \"recipe_analyzer\"},\n",
    "                    \"RecipeIngredientParts\": {\"type\": \"text\", \"analyzer\": \"recipe_analyzer\"},\n",
    "                    \"RecipeInstructions\": {\"type\": \"text\", \"analyzer\": \"recipe_analyzer\"},\n",
    "                    \"AggregatedRating\": {\"type\": \"float\"},\n",
    "                    \"ReviewCount\": {\"type\": \"float\"},\n",
    "                    \"Images\": {\"type\": \"keyword\"},\n",
    "                    \"text\": {\"type\": \"text\", \"analyzer\": \"recipe_analyzer\"}\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Create the index\n",
    "        app.es_client.indices.create(index=INDEX_NAME, body=settings)\n",
    "        print(f\"Created index {INDEX_NAME} with BM25 similarity\")\n",
    "    else:\n",
    "        print(f\"Index {INDEX_NAME} already exists\")\n",
    "\n",
    "def load_data_from_parquet():\n",
    "    \"\"\"Load data from Parquet file and index it in Elasticsearch in chunks\"\"\"\n",
    "    parquet_path = 'resource/csv/completed_recipes.parquet'\n",
    "    \n",
    "    if not os.path.exists(parquet_path):\n",
    "        print(f\"Parquet file not found at {parquet_path}\")\n",
    "        return False\n",
    "    \n",
    "    # Load the parquet file\n",
    "    df = pd.read_parquet(parquet_path)\n",
    "    print(f\"Loaded {len(df)} records from parquet file\")\n",
    "    \n",
    "    # Convert DataFrame to list of dicts for bulk indexing\n",
    "    records = df.to_dict(orient='records')\n",
    "    \n",
    "    # Process in chunks to avoid request size limits\n",
    "    chunk_size = 1000  # Process 1000 records at a time\n",
    "    total_chunks = (len(records) + chunk_size - 1) // chunk_size  # Ceiling division\n",
    "    \n",
    "    successful_chunks = 0\n",
    "    \n",
    "    for i in range(0, len(records), chunk_size):\n",
    "        chunk = records[i:i+chunk_size]\n",
    "        chunk_num = i // chunk_size + 1\n",
    "        \n",
    "        # Prepare bulk indexing operation for this chunk\n",
    "        bulk_data = []\n",
    "        for record in chunk:\n",
    "            # Clean up NaN values\n",
    "            cleaned_record = {k: ('' if pd.isna(v) else v) for k, v in record.items()}\n",
    "            \n",
    "            # Add index operation and document\n",
    "            bulk_data.append({\"index\": {\"_index\": INDEX_NAME}})\n",
    "            bulk_data.append(cleaned_record)\n",
    "        \n",
    "        # Perform bulk indexing for this chunk\n",
    "        try:\n",
    "            if bulk_data:\n",
    "                app.es_client.bulk(body=bulk_data, refresh=(chunk_num == total_chunks))\n",
    "                successful_chunks += 1\n",
    "                print(f\"Indexed chunk {chunk_num}/{total_chunks} ({len(chunk)} documents)\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error indexing chunk {chunk_num}: {str(e)}\")\n",
    "    \n",
    "    print(f\"Completed indexing: {successful_chunks}/{total_chunks} chunks successful\")\n",
    "    return successful_chunks > 0\n",
    "\n",
    "def pickle_index():\n",
    "    \"\"\"Pickle all the documents in the index for faster future loading\"\"\"\n",
    "    # Check if the index exists\n",
    "    if not app.es_client.indices.exists(index=INDEX_NAME):\n",
    "        print(f\"Index {INDEX_NAME} does not exist. Cannot pickle.\")\n",
    "        return False\n",
    "    \n",
    "    # Get all documents from the index\n",
    "    query = {\"query\": {\"match_all\": {}}, \"size\": 10000}  # Adjust size as needed\n",
    "    \n",
    "    try:\n",
    "        # Initialize scroll\n",
    "        result = app.es_client.search(\n",
    "            index=INDEX_NAME,\n",
    "            body=query,\n",
    "            scroll=\"2m\"  # Keep the search context alive for 2 minutes\n",
    "        )\n",
    "        \n",
    "        # Get the scroll ID\n",
    "        scroll_id = result[\"_scroll_id\"]\n",
    "        scroll_size = len(result[\"hits\"][\"hits\"])\n",
    "        \n",
    "        # Store all documents\n",
    "        all_docs = []\n",
    "        \n",
    "        # Get all documents\n",
    "        while scroll_size > 0:\n",
    "            all_docs.extend(result[\"hits\"][\"hits\"])\n",
    "            \n",
    "            # Scroll to next batch\n",
    "            result = app.es_client.scroll(scroll_id=scroll_id, scroll=\"2m\")\n",
    "            \n",
    "            # Update scroll ID and size\n",
    "            scroll_id = result[\"_scroll_id\"]\n",
    "            scroll_size = len(result[\"hits\"][\"hits\"])\n",
    "        \n",
    "        # Create directory if it doesn't exist\n",
    "        os.makedirs(os.path.dirname(PICKLE_PATH), exist_ok=True)\n",
    "        \n",
    "        # Pickle the documents and index settings\n",
    "        index_data = {\n",
    "            \"documents\": all_docs,\n",
    "            \"settings\": app.es_client.indices.get_settings(index=INDEX_NAME),\n",
    "            \"mappings\": app.es_client.indices.get_mapping(index=INDEX_NAME),\n",
    "            \"timestamp\": time.time()\n",
    "        }\n",
    "        \n",
    "        with open(PICKLE_PATH, 'wb') as f:\n",
    "            pickle.dump(index_data, f)\n",
    "        \n",
    "        print(f\"Pickled {len(all_docs)} documents to {PICKLE_PATH}\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error pickling index: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def load_index_from_pickle():\n",
    "    \"\"\"Load the index from a pickle file\"\"\"\n",
    "    if not os.path.exists(PICKLE_PATH):\n",
    "        print(f\"Pickle file not found at {PICKLE_PATH}\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        # Load the pickle file\n",
    "        with open(PICKLE_PATH, 'rb') as f:\n",
    "            index_data = pickle.load(f)\n",
    "        \n",
    "        print(f\"Loaded pickle file from {PICKLE_PATH}\")\n",
    "        \n",
    "        # Check if index exists and delete if it does\n",
    "        if app.es_client.indices.exists(index=INDEX_NAME):\n",
    "            app.es_client.indices.delete(index=INDEX_NAME)\n",
    "            print(f\"Deleted existing index {INDEX_NAME}\")\n",
    "        \n",
    "        # Create the index with the same settings and mappings\n",
    "        app.es_client.indices.create(\n",
    "            index=INDEX_NAME,\n",
    "            body={\n",
    "                \"settings\": index_data[\"settings\"][INDEX_NAME][\"settings\"],\n",
    "                \"mappings\": index_data[\"mappings\"][INDEX_NAME][\"mappings\"]\n",
    "            }\n",
    "        )\n",
    "        print(f\"Created index {INDEX_NAME} with pickled settings\")\n",
    "        \n",
    "        # Process in chunks to avoid request size limits\n",
    "        chunk_size = 1000  # Process 1000 records at a time\n",
    "        docs = index_data[\"documents\"]\n",
    "        total_chunks = (len(docs) + chunk_size - 1) // chunk_size  # Ceiling division\n",
    "        \n",
    "        successful_chunks = 0\n",
    "        \n",
    "        for i in range(0, len(docs), chunk_size):\n",
    "            chunk = docs[i:i+chunk_size]\n",
    "            chunk_num = i // chunk_size + 1\n",
    "            \n",
    "            # Prepare bulk indexing operation for this chunk\n",
    "            bulk_data = []\n",
    "            for doc in chunk:\n",
    "                bulk_data.append({\"index\": {\"_index\": INDEX_NAME, \"_id\": doc[\"_id\"]}})\n",
    "                bulk_data.append(doc[\"_source\"])\n",
    "            \n",
    "            # Perform bulk indexing for this chunk\n",
    "            try:\n",
    "                if bulk_data:\n",
    "                    app.es_client.bulk(body=bulk_data, refresh=(chunk_num == total_chunks))\n",
    "                    successful_chunks += 1\n",
    "                    print(f\"Indexed chunk {chunk_num}/{total_chunks} ({len(chunk)} documents) from pickle\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error indexing chunk {chunk_num} from pickle: {str(e)}\")\n",
    "        \n",
    "        print(f\"Completed indexing from pickle: {successful_chunks}/{total_chunks} chunks successful\")\n",
    "        return successful_chunks > 0\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading index from pickle: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "@app.route('/search', methods=['GET'])\n",
    "def search():\n",
    "    \"\"\"API endpoint for searching recipes\"\"\"\n",
    "    query = request.args.get('q', '')\n",
    "    size = int(request.args.get('size', 10))\n",
    "    \n",
    "    if not query:\n",
    "        return jsonify({\"error\": \"Query parameter 'q' is required\"}), 400\n",
    "    \n",
    "    # Perform BM25 search\n",
    "    search_body = {\n",
    "        \"size\": size,\n",
    "        \"query\": {\n",
    "            \"multi_match\": {\n",
    "                \"query\": query,\n",
    "                \"fields\": [\"Name^3\", \"Description^2\", \"Keywords^2\", \"RecipeIngredientParts\", \"RecipeInstructions\", \"text\"],\n",
    "                \"type\": \"best_fields\"\n",
    "            }\n",
    "        },\n",
    "        \"highlight\": {\n",
    "            \"fields\": {\n",
    "                \"Name\": {},\n",
    "                \"Description\": {},\n",
    "                \"RecipeIngredientParts\": {},\n",
    "                \"RecipeInstructions\": {}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = app.es_client.search(index=INDEX_NAME, body=search_body)\n",
    "        hits = response[\"hits\"][\"hits\"]\n",
    "        \n",
    "        results = []\n",
    "        for hit in hits:\n",
    "            source = hit[\"_source\"]\n",
    "            result = {\n",
    "                \"id\": source.get(\"RecipeId\", \"\"),\n",
    "                \"name\": source.get(\"Name\", \"\"),\n",
    "                \"description\": source.get(\"Description\", \"\"),\n",
    "                \"category\": source.get(\"RecipeCategory\", \"\"),\n",
    "                \"ingredients\": source.get(\"RecipeIngredientParts\", \"\"),\n",
    "                \"instructions\": source.get(\"RecipeInstructions\", \"\"),\n",
    "                \"rating\": source.get(\"AggregatedRating\", 0),\n",
    "                \"reviews\": source.get(\"ReviewCount\", 0),\n",
    "                \"image\": source.get(\"Images\", \"\"),\n",
    "                \"score\": hit[\"_score\"],\n",
    "                \"highlights\": hit.get(\"highlight\", {})\n",
    "            }\n",
    "            results.append(result)\n",
    "        \n",
    "        return jsonify({\n",
    "            \"total\": response[\"hits\"][\"total\"][\"value\"],\n",
    "            \"results\": results\n",
    "        })\n",
    "    \n",
    "    except Exception as e:\n",
    "        return jsonify({\"error\": str(e)}), 500\n",
    "\n",
    "@app.route('/recipe/<recipe_id>')\n",
    "def get_recipe(recipe_id):\n",
    "    \"\"\"API endpoint to get a specific recipe by ID\"\"\"\n",
    "    try:\n",
    "        search_body = {\n",
    "            \"query\": {\n",
    "                \"term\": {\n",
    "                    \"RecipeId\": recipe_id\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        response = app.es_client.search(index=INDEX_NAME, body=search_body)\n",
    "        hits = response[\"hits\"][\"hits\"]\n",
    "        \n",
    "        if not hits:\n",
    "            return jsonify({\"error\": \"Recipe not found\"}), 404\n",
    "        \n",
    "        source = hits[0][\"_source\"]\n",
    "        recipe = {\n",
    "            \"id\": source.get(\"RecipeId\", \"\"),\n",
    "            \"name\": source.get(\"Name\", \"\"),\n",
    "            \"description\": source.get(\"Description\", \"\"),\n",
    "            \"category\": source.get(\"RecipeCategory\", \"\"),\n",
    "            \"ingredients\": source.get(\"RecipeIngredientParts\", \"\"),\n",
    "            \"instructions\": source.get(\"RecipeInstructions\", \"\"),\n",
    "            \"rating\": source.get(\"AggregatedRating\", 0),\n",
    "            \"reviews\": source.get(\"ReviewCount\", 0),\n",
    "            \"image\": source.get(\"Images\", \"\")\n",
    "        }\n",
    "        \n",
    "        return jsonify(recipe)\n",
    "    \n",
    "    except Exception as e:\n",
    "        return jsonify({\"error\": str(e)}), 500\n",
    "\n",
    "@app.route('/initialize', methods=['GET', 'POST'])\n",
    "def initialize():\n",
    "    \"\"\"Initialize the Elasticsearch index and load data\"\"\"\n",
    "    try:\n",
    "        # Delete existing index if it exists to start clean\n",
    "        if app.es_client.indices.exists(index=INDEX_NAME):\n",
    "            app.es_client.indices.delete(index=INDEX_NAME)\n",
    "            print(f\"Deleted existing index {INDEX_NAME}\")\n",
    "            \n",
    "        # Check if pickle file exists\n",
    "        if os.path.exists(PICKLE_PATH):\n",
    "            print(\"Pickle file found. Loading index from pickle...\")\n",
    "            success = load_index_from_pickle()\n",
    "            if success:\n",
    "                return jsonify({\n",
    "                    \"status\": \"success\", \n",
    "                    \"message\": \"Index loaded from pickle successfully\"\n",
    "                })\n",
    "            else:\n",
    "                # If loading from pickle fails, try creating from parquet\n",
    "                print(\"Failed to load from pickle. Trying to create from parquet...\")\n",
    "        \n",
    "        # Create from parquet\n",
    "        create_index()\n",
    "        success = load_data_from_parquet()\n",
    "        \n",
    "        if success:\n",
    "            # After successful loading, pickle the index for future use\n",
    "            pickle_success = pickle_index()\n",
    "            pickle_msg = \" and pickled for future use\" if pickle_success else \" but failed to pickle\"\n",
    "            \n",
    "            return jsonify({\n",
    "                \"status\": \"success\", \n",
    "                \"message\": f\"Index created and data loaded successfully{pickle_msg}\"\n",
    "            })\n",
    "        else:\n",
    "            return jsonify({\n",
    "                \"status\": \"error\", \n",
    "                \"message\": \"Failed to load data from parquet\"\n",
    "            }), 500\n",
    "    \n",
    "    except Exception as e:\n",
    "        return jsonify({\"status\": \"error\", \"message\": str(e)}), 500\n",
    "\n",
    "# Check if index exists and initialize if needed\n",
    "if not app.es_client.indices.exists(index=INDEX_NAME):\n",
    "    print(\"Index doesn't exist. Initializing...\")\n",
    "    initialize()\n",
    "else:\n",
    "    print(f\"Index {INDEX_NAME} already exists with {app.es_client.count(index=INDEX_NAME)['count']} documents\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n"
     ]
    }
   ],
   "source": [
    "app.run(debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
